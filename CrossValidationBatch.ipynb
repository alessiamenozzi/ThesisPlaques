{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "# Import libraries\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import random\n",
    "random.seed(seed)\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_curve, roc_auc_score, precision_recall_curve, auc, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile, f_classif, f_regression, SelectFromModel\n",
    "sc = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caricamento Dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: [0 1 1 0 0 0 1 0 0 1 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0 1\n",
      " 1 0 0 1 0 1 1 1 1 1 0 0 0 1 0 0 1 0 0 0 0 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0\n",
      " 1 1 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 1 1 1 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0\n",
      " 0 0 0 0 1 1 0 1 1]\n",
      "Number of labels: 120\n",
      "[[-7.82822844e+02 -1.02400000e+03  3.07100000e+03 ...  2.69972505e+03\n",
      "   2.80963106e-02  1.00514866e+01]\n",
      " [-9.66090232e+02 -3.02400000e+03  3.07100000e+03 ...  5.06730171e+03\n",
      "   4.13964322e-02  9.21642811e+00]\n",
      " [-8.96630635e+02 -3.02400000e+03  3.07100000e+03 ...  3.95715175e+03\n",
      "   1.13273940e-02  2.75076797e+01]\n",
      " ...\n",
      " [-9.39521199e+02 -3.02400000e+03  3.07100000e+03 ...  2.38010395e+03\n",
      "   3.21141803e-02  3.22969245e+00]\n",
      " [-6.02441998e+02 -1.02400000e+03  3.07100000e+03 ...  1.36263582e+02\n",
      "   6.99679621e-02  4.97074683e-01]\n",
      " [-5.15162265e+02 -1.02400000e+03  3.07100000e+03 ...  4.02397749e+03\n",
      "   7.17804108e-02  4.97083572e+00]]\n",
      "(120, 112)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "file_path = \"/Users/alessiamenozzi/Desktop/ThesisPlaques/data_rad_clin_DEF.csv\"\n",
    "#file_path = \"C:\\\\Users\\\\bsbar\\\\Desktop\\\\Tesi\\\\ThesisPlaques\\\\data_rad_clin_DEF.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Lista degli ID da escludere\n",
    "ids_to_exclude = [\"patient_TC_19\", \"patient_TC_40\", \"patient_TC_88\", \"patient_TC_150\", \"patient_TC_193\", \"patient_TC_200\", \"patient_TC_17\", \"patient_TC_107\", \"patient_TC_127\" ]\n",
    "\n",
    "\n",
    "# Filtra il DataFrame per escludere le righe con gli ID specificati\n",
    "filtered_data = data[~data['IDs_new'].isin(ids_to_exclude)]\n",
    "\n",
    "# Estrae i valori dalla colonna 'label' del DataFrame filtrato\n",
    "labels_column = filtered_data['label']\n",
    "\n",
    "# Converte i valori della colonna 'label' in numeri interi\n",
    "labels = labels_column.astype(int).tolist()\n",
    "\n",
    "Y_train=np.array(labels)\n",
    "print(\"Labels:\", Y_train)\n",
    "print(\"Number of labels:\", len(Y_train))\n",
    "\n",
    "file_path = \"/Users/alessiamenozzi/Desktop/ThesisPlaques/features_radiomiche.csv\" # Sostituisci con il percorso corretto\n",
    "df = pd.read_csv(file_path, sep=';')\n",
    "\n",
    "# Verifica i nomi delle colonne\n",
    "\n",
    "# Estrai i valori dalla colonna 'Paziente'\n",
    "labels = df['Paziente'].astype(int)\n",
    "\n",
    "# Rimuovi la colonna 'Paziente' per ottenere solo le feature\n",
    "df_features = df.drop(columns=['Paziente'])\n",
    "\n",
    "# Converti le features in un array numpy\n",
    "x_train = df_features.to_numpy()\n",
    "\n",
    "print(x_train)\n",
    "print(x_train.shape)  # (120, num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funzioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_correlation(X, threshold=0.85):\n",
    "    corr_matrix = np.corrcoef(X, rowvar=False)\n",
    "    upper_triangle = np.triu(corr_matrix, k=1)\n",
    "    to_drop = [column for column in range(upper_triangle.shape[0]) if any(abs(upper_triangle[column, :]) > threshold)]\n",
    "    return to_drop\n",
    "\n",
    "\n",
    "def remove_highly_correlated_features(X, threshold=0.85):\n",
    "    corr_matrix = np.corrcoef(X, rowvar=False)\n",
    "    upper_triangle = np.triu(corr_matrix, k=1)\n",
    "    to_drop = [column for column in range(upper_triangle.shape[0]) if any(abs(upper_triangle[column, :]) > threshold)]\n",
    "    X_reduced = np.delete(X, to_drop, axis=1)\n",
    "    return X_reduced, to_drop\n",
    "\n",
    "\n",
    "def remove_high_pvalue_features(X, y, alpha=0.05):\n",
    "    selector = SelectKBest(score_func=f_classif, k='all')\n",
    "    selector.fit(X, y)\n",
    "    p_values = selector.pvalues_\n",
    "    features_to_keep = np.where(p_values < alpha)[0]\n",
    "    X_reduced = X[:, features_to_keep]\n",
    "    return X_reduced, features_to_keep\n",
    "\n",
    "## FEATURE SELECTION LASSO\n",
    "def select_features_with_lasso(X, y, alpha=0.001):\n",
    "    \n",
    "    # Fit Lasso regression model\n",
    "    lasso = Lasso(alpha=alpha)\n",
    "    lasso.fit(X, y)\n",
    "\n",
    "    # Get coefficients\n",
    "    coefficients = lasso.coef_\n",
    "\n",
    "    # Select features with non-zero coefficients\n",
    "    selected_features = np.where(coefficients != 0)[0]\n",
    "\n",
    "    # Create new feature matrix with only selected features\n",
    "    X_selected = X[:, selected_features]\n",
    "\n",
    "    return X_selected, selected_features\n",
    "\n",
    "## FEATURE SELECTION LOGISTIC\n",
    "def logistic_regression_feature_selection(X, y, num_features):\n",
    "    lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    lr.fit(X, y)\n",
    "    coef_abs = np.abs(lr.coef_)\n",
    "    feature_importances = np.mean(coef_abs, axis=0)\n",
    "    selected_features = feature_importances.argsort()[-num_features:][::-1]\n",
    "    # Create new feature matrix with only selected features\n",
    "    X_selected = X[:, selected_features]\n",
    "    return X_selected, selected_features\n",
    "\n",
    "\n",
    "def mrmr_feature_selection(X, y, num_features):\n",
    "    # Calcolare l'informazione mutua tra ogni caratteristica e il target\n",
    "    mi = mutual_info_classif(X, y)\n",
    "    \n",
    "    # Standardizzare le caratteristiche\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Calcolare la distanza euclidea tra le caratteristiche\n",
    "    distances = squareform(pdist(X_scaled.T, 'euclidean'))\n",
    "    \n",
    "    selected_features = []\n",
    "    selected_indices = []\n",
    "    \n",
    "    # Selezionare la prima caratteristica con la massima informazione mutua\n",
    "    first_feature_index = np.argmax(mi)\n",
    "    selected_features.append(first_feature_index)\n",
    "    selected_indices.append(first_feature_index)\n",
    "    \n",
    "    # Iterare per selezionare le caratteristiche rimanenti\n",
    "    for _ in range(num_features - 1):\n",
    "        max_relevance = -np.inf\n",
    "        selected_feature_index = -1\n",
    "        \n",
    "        for i in range(X.shape[1]):\n",
    "            if i in selected_indices:\n",
    "                continue\n",
    "            \n",
    "            relevance = mi[i]\n",
    "            redundancy = np.mean(distances[i, selected_indices])\n",
    "            \n",
    "            mrmr_score = relevance - redundancy\n",
    "            \n",
    "            if mrmr_score > max_relevance:\n",
    "                max_relevance = mrmr_score\n",
    "                selected_feature_index = i\n",
    "        \n",
    "        selected_features.append(selected_feature_index)\n",
    "        selected_indices.append(selected_feature_index)\n",
    "\n",
    "    X_selected = X[:, selected_indices]\n",
    "    return X_selected, selected_indices\n",
    "\n",
    "\n",
    "\n",
    "## FEATURE SELECTION RANDOM FOREST\n",
    "def rf_feature_selection(X, y, num_features):\n",
    "    # Inizializza il classificatore Random Forest\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    # Addestra il modello\n",
    "    rf.fit(X, y)\n",
    "    # Ottieni l'importanza delle caratteristiche\n",
    "    feature_importances = rf.feature_importances_\n",
    "    # Seleziona gli indici delle caratteristiche pi√π importanti\n",
    "    selected_features = np.argsort(feature_importances)[-num_features:][::-1]\n",
    "    # Crea una nuova matrice di caratteristiche con solo le caratteristiche selezionate\n",
    "    X_selected = X[:, selected_features]\n",
    "    return X_selected, selected_features\n",
    "\n",
    "\n",
    "def p_value_feature_selection(X, num_features):\n",
    "    \"\"\"\n",
    "    Seleziona le prime `num_features` caratteristiche dal vettore di caratteristiche.\n",
    "\n",
    "    Args:\n",
    "    X (np.ndarray): Matrice delle caratteristiche (numero di campioni, numero di caratteristiche).\n",
    "    num_features (int): Numero di caratteristiche da selezionare.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: Nuova matrice delle caratteristiche con solo le caratteristiche selezionate.\n",
    "    np.ndarray: Indici delle caratteristiche selezionate.\n",
    "    \"\"\"\n",
    "    # Controlla se num_features √® maggiore del numero totale di caratteristiche\n",
    "    if num_features > X.shape[1]:\n",
    "        raise ValueError(f\"num_features ({num_features}) √® maggiore del numero totale di caratteristiche ({X.shape[1]})\")\n",
    "\n",
    "    # Seleziona i primi num_features indici\n",
    "    selected_features = np.arange(num_features)\n",
    "    \n",
    "    # Crea una nuova matrice di caratteristiche con solo le caratteristiche selezionate\n",
    "    X_selected = X[:, selected_features]\n",
    "    \n",
    "    return X_selected, selected_features\n",
    "\n",
    "## FUNZIONE PER RIMUOVERE FEATURES SELEZIONATE\n",
    "def filter_patients_features(filtered_patients, selected_features):\n",
    "    \"\"\"\n",
    "    Removes the non-selected features from the filtered_patients array.\n",
    "\n",
    "    Parameters:\n",
    "    filtered_patients (list of numpy.ndarray): The list containing patients' images' features.\n",
    "    selected_features (numpy.ndarray): The indices of the selected features.\n",
    "\n",
    "    Returns:\n",
    "    list of numpy.ndarray: The new filtered_patients array with only the selected features.\n",
    "    \"\"\"\n",
    "    filtered_patients_selected = []\n",
    "\n",
    "    for patient_features in filtered_patients:\n",
    "        # Select only the features specified in selected_features\n",
    "        patient_features_selected = patient_features[:, selected_features]\n",
    "        filtered_patients_selected.append(patient_features_selected)\n",
    "\n",
    "    return filtered_patients_selected\n",
    "\n",
    "\n",
    "def select_features_by_p_value(x_train_expanded, y_train_expanded, p_value_threshold=0.01):\n",
    "\n",
    "    p_values = []\n",
    "    num_features = x_train_expanded.shape[1]\n",
    "\n",
    "    for i in range(num_features):\n",
    "        feature = x_train_expanded[:, i]\n",
    "        group_0 = feature[y_train_expanded == 0]\n",
    "        group_1 = feature[y_train_expanded == 1]\n",
    "        t_stat, p_val = ttest_ind(group_0, group_1, equal_var=False)\n",
    "        p_values.append(p_val)\n",
    "\n",
    "    # Convertire i p-value in un array numpy per ordinare pi√π facilmente\n",
    "    p_values = np.array(p_values)\n",
    "\n",
    "    # Selezionare le caratteristiche con p-value < soglia\n",
    "    selected_features_indices = np.where(p_values < p_value_threshold)[0]\n",
    "\n",
    "    # Ordinare le caratteristiche selezionate in base ai p-value\n",
    "    sorted_indices = selected_features_indices[np.argsort(p_values[selected_features_indices])]\n",
    "\n",
    "    x_train_expanded = x_train_expanded[:, sorted_indices]\n",
    "\n",
    "    return x_train_expanded, sorted_indices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def classification_method(selector, classifier, alpha, x_train_expanded, y_train_expanded, x_test, y_test, num_features, mode=\"Val\", selected_features=[0], thresholds=np.arange(0.3, 0.6, 0.01)):\n",
    "    best_f1_score = 0\n",
    "    best_case = None\n",
    "\n",
    "    if mode == \"Val\":\n",
    "        selected_features = None  # Inizializziamo selected_features per prevenire l'errore UnboundLocalError\n",
    "\n",
    "        if num_features != len(x_train_expanded[0]) or alpha != 0:\n",
    "            if selector == \"lasso\":\n",
    "                X_selected, selected_features = select_features_with_lasso(x_train_expanded, y_train_expanded, alpha)\n",
    "            elif selector == \"logistic\":\n",
    "                X_selected, selected_features = logistic_regression_feature_selection(x_train_expanded, y_train_expanded, num_features)\n",
    "            elif selector == \"mrmr\":\n",
    "                X_selected, selected_features = mrmr_feature_selection(x_train_expanded, y_train_expanded, num_features)\n",
    "            elif selector == \"rf\":\n",
    "                X_selected, selected_features = rf_feature_selection(x_train_expanded, y_train_expanded, num_features)\n",
    "            elif selector == \"p_value\":\n",
    "                X_selected, selected_features = p_value_feature_selection(x_train_expanded, num_features)\n",
    "            else:\n",
    "                print(\"Wrong selector. Choose between: mrmr, rf, logistic, lasso\")\n",
    "                return\n",
    "\n",
    "            x_test = x_test[:, selected_features]  # Applichiamo la selezione delle feature anche su x_test\n",
    "        else:\n",
    "            X_selected = x_train_expanded\n",
    "            selected_features = list(range(len(x_train_expanded[0])))  # Selezioniamo tutte le feature se non si fa feature selection\n",
    "\n",
    "        number_features = len(selected_features)  # Numero di feature selezionate\n",
    "\n",
    "        # Addestriamo il classificatore\n",
    "        classifier.fit(X_selected, y_train_expanded)\n",
    "\n",
    "\n",
    "    if (mode == \"Test\"):\n",
    "        x_test = x_test[:, selected_features]\n",
    "        number_features = len(selected_features)\n",
    "    \n",
    "        # Prevediamo le probabilit√† sul set di test\n",
    "    y_proba_test = classifier.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    if(isinstance(thresholds, np.ndarray)== False):\n",
    "        thresholds=[thresholds]\n",
    "        \n",
    "    \n",
    "    for threshold in thresholds:\n",
    "            # Previsioni usando la soglia custom\n",
    "            y_pred_custom_test = (y_proba_test >= threshold).astype(int)\n",
    "\n",
    "            # Calcolo metriche\n",
    "            accuracy = accuracy_score(y_test, y_pred_custom_test)\n",
    "            f1 = f1_score(y_test, y_pred_custom_test)\n",
    "            roc_auc = roc_auc_score(y_test, y_proba_test)\n",
    "\n",
    "            # Precision-recall curve e AUC\n",
    "            precision, recall, _ = precision_recall_curve(y_test, y_proba_test)\n",
    "            pr_auc = auc(recall, precision)\n",
    "\n",
    "            # Miglior precisione e richiamo (basato su soglia personalizzata)\n",
    "            best_precision = precision[np.argmax(recall)]\n",
    "            best_recall = recall[np.argmax(recall)]\n",
    "\n",
    "            # Matrice di confusione\n",
    "            conf = confusion_matrix(y_test, y_pred_custom_test)\n",
    "\n",
    "            # Se il nuovo risultato √® migliore rispetto al migliore attuale, aggiorniamo\n",
    "            if f1 > best_f1_score or (f1 == best_f1_score and pr_auc > (best_case['pr_auc'] if best_case else 0)):\n",
    "                best_f1_score = f1\n",
    "                best_case = {\n",
    "                    'alpha': alpha,\n",
    "                    'num_features': number_features,\n",
    "                    'selected_features': selected_features,\n",
    "                    'pr_auc': pr_auc,\n",
    "                    'best_precision': best_precision,\n",
    "                    'best_recall': best_recall,\n",
    "                    'roc_auc': roc_auc,\n",
    "                    'f1': f1,\n",
    "                    'accuracy': accuracy,\n",
    "                    'confusion_matrix': conf,\n",
    "                    'best_threshold': threshold\n",
    "                }\n",
    "\n",
    "    return best_case\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train1, y_test, x_train1, X_test= train_test_split(Y_train, x_train, test_size=0.2, shuffle=False, random_state=1)\n",
    "#y_train, y_val, X_train, X_val= train_test_split(y_train1, x_train1, test_size=0.3, shuffle=True, stratify=y_train1, random_state=2)\n",
    "#y_train, y_val, X_train, X_val= train_test_split(y_train1, x_train1, test_size=0.3, shuffle=False, random_state=7)\n",
    "Y_train, y_test, X_train, X_test= train_test_split(Y_train, x_train, test_size=0.2, shuffle=False, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature correlation and p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 112)\n",
      "(24, 112)\n",
      "(96, 112)\n",
      "(24, 112)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train_reduced, dropped_features = remove_highly_correlated_features(X_train, 1)\n",
    "\n",
    "# Riduci X_val e X_test usando le feature rimosse\n",
    "#X_val_reduced = np.delete(X_val, dropped_features, axis=1)\n",
    "X_test_reduced = np.delete(X_test, dropped_features, axis=1)\n",
    "\n",
    "print(X_train_reduced.shape)\n",
    "\n",
    "print(X_test_reduced.shape)\n",
    "#print(X_val_reduced.shape)\n",
    "\n",
    "\n",
    "\n",
    "# Rimozione delle feature con p-value elevato\n",
    "#X_train_reduced, features_to_keep = remove_high_pvalue_features(X_train_reduced, y_train, alpha=0.05)\n",
    "#print(features_to_keep)\n",
    "#X_val_reduced = X_val_reduced[:, features_to_keep]\n",
    "#X_test_reduced = X_test_reduced[:, features_to_keep]\n",
    "\n",
    "\n",
    "print(X_train_reduced.shape)\n",
    "\n",
    "print(X_test_reduced.shape)\n",
    "#print(X_val_reduced.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parametri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_values = np.linspace(0, 0.006, 30).tolist()\n",
    "alpha=0.003\n",
    "\n",
    "thresholds=np.arange(0.001, 0.501, 0.001) \n",
    "\n",
    "#selectors=['p_value', 'mrmr','rf', 'logistic', 'lasso']\n",
    "selectors=['p_value', 'mrmr','rf', 'logistic']\n",
    "classifiers=['XgBoost', 'MLP', 'SVM', 'ensemble','RandomForest', 'Logistic']\n",
    "#classifiers=['RandomForest', 'XgBoost', 'MLP', 'SVM']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 112)\n",
      "(96,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_reduced.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with fold: 0\n",
      "X_train_reduced.shape (96, 112) Y_train.shape  (96,)\n",
      "Starting with classifier: XgBoost\n",
      "Starting with selector: p_value\n",
      "Starting with selector: mrmr\n",
      "Starting with selector: rf\n",
      "Starting with selector: logistic\n",
      "Starting with classifier: MLP\n",
      "Starting with selector: p_value\n",
      "Starting with selector: mrmr\n",
      "Starting with selector: rf\n",
      "Starting with selector: logistic\n",
      "Starting with classifier: SVM\n",
      "Starting with selector: p_value\n",
      "Starting with selector: mrmr\n",
      "Starting with selector: rf\n",
      "Starting with selector: logistic\n",
      "Starting with classifier: ensemble\n",
      "Starting with selector: p_value\n",
      "Starting with selector: mrmr\n",
      "Starting with selector: rf\n",
      "Starting with selector: logistic\n",
      "Starting with classifier: RandomForest\n",
      "Starting with selector: p_value\n",
      "Starting with selector: mrmr\n",
      "Starting with selector: rf\n",
      "Starting with selector: logistic\n",
      "Starting with classifier: Logistic\n",
      "Starting with selector: p_value\n",
      "Starting with selector: mrmr\n",
      "Starting with selector: rf\n",
      "Starting with selector: logistic\n",
      "Starting with fold: 1\n",
      "X_train_reduced.shape (96, 112) Y_train.shape  (96,)\n",
      "Starting with classifier: XgBoost\n",
      "Starting with selector: p_value\n",
      "Starting with selector: mrmr\n",
      "Starting with selector: rf\n",
      "Starting with selector: logistic\n",
      "Starting with classifier: MLP\n",
      "Starting with selector: p_value\n",
      "Starting with selector: mrmr\n",
      "Starting with selector: rf\n",
      "Starting with selector: logistic\n",
      "Starting with classifier: SVM\n",
      "Starting with selector: p_value\n",
      "Starting with selector: mrmr\n",
      "Starting with selector: rf\n",
      "Starting with selector: logistic\n",
      "Starting with classifier: ensemble\n",
      "Starting with selector: p_value\n",
      "Starting with selector: mrmr\n",
      "Starting with selector: rf\n",
      "Starting with selector: logistic\n",
      "Starting with classifier: RandomForest\n",
      "Starting with selector: p_value\n",
      "Starting with selector: mrmr\n",
      "Starting with selector: rf\n",
      "Starting with selector: logistic\n",
      "Starting with classifier: Logistic\n",
      "Starting with selector: p_value\n",
      "Starting with selector: mrmr\n",
      "Starting with selector: rf\n",
      "Starting with selector: logistic\n",
      "Starting with fold: 2\n",
      "X_train_reduced.shape (96, 112) Y_train.shape  (96,)\n",
      "Starting with classifier: XgBoost\n",
      "Starting with selector: p_value\n",
      "Starting with selector: mrmr\n",
      "Starting with selector: rf\n",
      "Starting with selector: logistic\n",
      "Starting with classifier: MLP\n",
      "Starting with selector: p_value\n",
      "Starting with selector: mrmr\n"
     ]
    }
   ],
   "source": [
    "n_folds=5\n",
    "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "scores = []\n",
    "\n",
    "# Lista per salvare i risultati\n",
    "results_val = [[{} for _ in range(len(classifiers))] for _ in range(n_folds)]\n",
    "\n",
    "# Inizia la cross-validation\n",
    "for fold_idx, (train_index, val_index) in enumerate(skf.split(X_train_reduced, Y_train)):\n",
    "    print(\"Starting with fold:\", fold_idx)\n",
    "    print(\"X_train_reduced.shape\", X_train_reduced.shape, \"Y_train.shape \", Y_train.shape)\n",
    "    x_train_reduced, X_val_reduced = X_train_reduced[train_index], X_train_reduced[val_index]\n",
    "    y_train, y_val = Y_train[train_index], Y_train[val_index]\n",
    "\n",
    "    for i, classifier in enumerate(classifiers):\n",
    "        print(\"Starting with classifier:\", classifier)\n",
    "        for j, selector in enumerate(selectors):\n",
    "            print(\"Starting with selector:\", selector)\n",
    "\n",
    "            results_val[fold_idx][i][j] = {\n",
    "                'fold': fold_idx,\n",
    "                'classifier': classifier,\n",
    "                'selector': selector,\n",
    "                'alpha': [],\n",
    "                'num_features': [],\n",
    "                'pr_auc': [],\n",
    "                'best_precision': [],\n",
    "                'best_recall': [],\n",
    "                'roc_auc': [],\n",
    "                'f1': [],\n",
    "                'accuracy': [],\n",
    "                'confusion_matrix': [],\n",
    "                'best_threshold': [],\n",
    "                'selected_features': []\n",
    "            }\n",
    "\n",
    "            best_f1 = 0\n",
    "            best_case = None\n",
    "\n",
    "            limit=len(x_train_reduced[0]) + 1\n",
    "            for t in range(2, limit):\n",
    "                # Seleziona le feature ridotte\n",
    "                #print(\"Number of features \", t)\n",
    "\n",
    "                # Selezione del classificatore\n",
    "                if classifier == 'RandomForest':\n",
    "                    classi = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "                    selector_rf = SelectFromModel(classi, threshold='mean')\n",
    "                    X_train_rf = selector_rf.fit_transform(x_train_reduced, y_train)\n",
    "                    X_test_rf = selector_rf.transform(X_test_reduced)\n",
    "                    X_val_rf = selector_rf.transform(X_val_reduced)\n",
    "                    if(t>len(X_train_rf[0])):\n",
    "                        continue\n",
    "\n",
    "                elif classifier == 'Logistic':\n",
    "                    classi = LogisticRegression()\n",
    "                    selector_lr = SelectFromModel(classi, threshold='mean')\n",
    "                    X_train_lr = selector_lr.fit_transform(x_train_reduced, y_train)\n",
    "                    X_test_lr = selector_lr.transform(X_test_reduced)\n",
    "                    X_val_lr = selector_lr.transform(X_val_reduced)\n",
    "                    if(t>len(X_train_lr[0])):\n",
    "                        continue\n",
    "                    \n",
    "                elif classifier == 'SVM':\n",
    "                    classi = SVC(kernel='rbf', probability=True, random_state=42)\n",
    "                elif classifier == 'XgBoost':\n",
    "                    classi = XGBClassifier()\n",
    "                elif classifier == 'MLP':\n",
    "                    classi = MLPClassifier(hidden_layer_sizes=(128, 64, 32), max_iter=1000, random_state=42, early_stopping=True, learning_rate='adaptive')\n",
    "                elif classifier == 'ensemble':\n",
    "                    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "                    logistic_model = LogisticRegression(random_state=42)\n",
    "                    svc_model = SVC(kernel='rbf', probability=True, random_state=42)\n",
    "\n",
    "                    # Crea l'ensemble con VotingClassifier\n",
    "                    classi = VotingClassifier(\n",
    "                        estimators=[\n",
    "                            ('random_forest', rf_model),\n",
    "                            ('logistic', logistic_model),\n",
    "                            ('svc', svc_model)\n",
    "                        ],\n",
    "                        voting='soft'  # 'soft' usa le probabilit√† di classe, 'hard' usa le predizioni di classe\n",
    "                        )\n",
    "\n",
    "                \n",
    "                if classifier == 'RandomForest':\n",
    "                    best_case = classification_method(selector, classi, alpha, X_train_rf, y_train, X_val_rf, y_val, num_features=t, mode=\"Val\", selected_features=[0])\n",
    "                elif classifier == 'Logistic':\n",
    "                    best_case = classification_method(selector, classi, alpha, X_train_lr, y_train, X_val_lr, y_val, num_features=t, mode=\"Val\", selected_features=[0])\n",
    "                else:\n",
    "                    best_case = classification_method(selector, classi, alpha, x_train_reduced, y_train, X_val_reduced, y_val, num_features=t, mode=\"Val\", selected_features=[0])\n",
    "\n",
    "\n",
    "                if best_case:\n",
    "                    results_val[fold_idx][i][j]['alpha'].append(best_case['alpha'])\n",
    "                    results_val[fold_idx][i][j]['selected_features'].append(best_case['selected_features'])\n",
    "                    results_val[fold_idx][i][j]['num_features'].append(best_case['num_features'])\n",
    "                    results_val[fold_idx][i][j]['pr_auc'].append(best_case['pr_auc'])\n",
    "                    results_val[fold_idx][i][j]['best_precision'].append(best_case['best_precision'])\n",
    "                    results_val[fold_idx][i][j]['best_recall'].append(best_case['best_recall'])\n",
    "                    results_val[fold_idx][i][j]['roc_auc'].append(best_case['roc_auc'])\n",
    "                    results_val[fold_idx][i][j]['f1'].append(best_case['f1'])\n",
    "                    results_val[fold_idx][i][j]['accuracy'].append(best_case['accuracy'])\n",
    "                    results_val[fold_idx][i][j]['confusion_matrix'].append(best_case['confusion_matrix'])\n",
    "                    results_val[fold_idx][i][j]['best_threshold'].append(best_case['best_threshold'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La migliore configurazione √®:\n",
      "Classifier: Logistic, Selector: p_value, Num Features: 5.7, F1: 0.6829641458568187, PR AUC: 0.6817777203775134, Best Threshold: 0.4004285714285715\n",
      "\n",
      "La seconda migliore configurazione √®:\n",
      "Classifier: Logistic, Selector: logistic, Num Features: 5.7, F1: 0.6772910863226966, PR AUC: 0.6659583517206, Best Threshold: 0.3974285714285716\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "diz = [[{} for _ in range(len(selectors))] for _ in range(len(classifiers))]\n",
    "\n",
    "for i, classifier in enumerate(classifiers):\n",
    "    for j, selector in enumerate(selectors):\n",
    "        # Inizializza un dizionario per contenere le medie dei fold\n",
    "        combined_result = {\n",
    "            'classifier': classifier,\n",
    "            'selector': selector,\n",
    "            'alpha': [],\n",
    "            'num_features': [],\n",
    "            'pr_auc': [],\n",
    "            'best_precision': [],\n",
    "            'best_recall': [],\n",
    "            'roc_auc': [],\n",
    "            'f1': [],\n",
    "            'accuracy': [],\n",
    "            'best_threshold': [],\n",
    "            # Lasciamo selected_features e confusion_matrix da parte\n",
    "            'selected_features': [],  # Lasciamo come lista vuota per ora\n",
    "            'confusion_matrix': []    # Lasciamo come lista vuota per ora\n",
    "        }\n",
    "        \n",
    "        # Per ogni fold, aggiungi i valori corrispondenti per calcolare la media\n",
    "        for fold_idx in range(n_folds):\n",
    "            for key in results_val[fold_idx][i][j].keys():\n",
    "                if key in ['classifier', 'selector', 'selected_features', 'confusion_matrix']:  \n",
    "                    # Non calcoliamo la media su 'selected_features' e 'confusion_matrix'\n",
    "                    continue\n",
    "                if key not in combined_result:\n",
    "                    combined_result[key] = []\n",
    "                \n",
    "                # Aggiungi la media dei valori del fold\n",
    "                combined_result[key].append(np.mean(results_val[fold_idx][i][j][key]))\n",
    "        \n",
    "        # Calcola la media per ogni chiave e salva nel dizionario finale\n",
    "        for key in combined_result.keys():\n",
    "            if key not in ['classifier', 'selector', 'selected_features', 'confusion_matrix']:\n",
    "                combined_result[key] = np.mean(combined_result[key])\n",
    "        \n",
    "        # Lascia selected_features e confusion_matrix vuoti o trattali diversamente se necessario\n",
    "        combined_result['selected_features'] = \"N/A\"  # O qualsiasi altro trattamento\n",
    "        combined_result['confusion_matrix'] = \"N/A\"   # O qualsiasi altro trattamento\n",
    "        \n",
    "        # Salva il risultato medio nel dizionario generale\n",
    "        diz[i][j] = combined_result\n",
    "\n",
    "# Lista per contenere le configurazioni migliori\n",
    "best_configurations = []\n",
    "\n",
    "# Cerca le due configurazioni migliori, prima ordinando per f1 e poi per pr_auc\n",
    "for i in range(len(classifiers)):\n",
    "    for j in range(len(selectors)):\n",
    "        result = diz[i][j]\n",
    "        best_configurations.append((result['f1'], result['pr_auc'], result['classifier'], result['selector'], result['num_features'], result['best_threshold']))\n",
    "\n",
    "# Ordina le configurazioni in base all'f1 e in caso di parit√† al pr_auc\n",
    "best_configurations = sorted(best_configurations, key=lambda x: (-x[0], -x[1]))  # Ordina in modo decrescente per f1 e pr_auc\n",
    "\n",
    "# Estrai le due migliori configurazioni\n",
    "best_config_1 = best_configurations[0]\n",
    "best_config_2 = best_configurations[1]\n",
    "\n",
    "\n",
    "print(\"La migliore configurazione √®:\")\n",
    "print(f\"Classifier: {best_config_1[2]}, Selector: {best_config_1[3]}, Num Features: {best_config_1[4]}, F1: {best_config_1[0]}, PR AUC: {best_config_1[1]}, Best Threshold: {best_config_1[5]}\")\n",
    "\n",
    "print(\"\\nLa seconda migliore configurazione √®:\")\n",
    "print(f\"Classifier: {best_config_2[2]}, Selector: {best_config_2[3]}, Num Features: {best_config_2[4]}, F1: {best_config_2[0]}, PR AUC: {best_config_2[1]}, Best Threshold: {best_config_2[5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "## qui applico la classificazione usando i parametri miglori per√≤ bisogna capire come trattare il numero di features e la best_threshold \n",
    "\n",
    "\n",
    "classi = LogisticRegression()\n",
    "selector_lr = SelectFromModel(classi, threshold='mean')\n",
    "X_train_lr = selector_lr.fit_transform(x_train_reduced, y_train)\n",
    "X_test_lr = selector_lr.transform(X_test_reduced)\n",
    "\n",
    "\n",
    "best_case = classification_method('p_value', classi, alpha, X_train_lr, y_train, X_test_lr, y_test, num_features=5, mode=\"Val\", selected_features=[0], thresholds=best_config_1[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics from best_case:\n",
      "Alpha: 0.003\n",
      "Number of Features: 5\n",
      "Selected Features: [0 1 2 3 4]\n",
      "Precision-Recall AUC: 0.7901350482377959\n",
      "Best Precision: 0.3333333333333333\n",
      "Best Recall: 1.0\n",
      "ROC AUC: 0.796875\n",
      "F1 Score: 0.631578947368421\n",
      "Accuracy: 0.7083333333333334\n",
      "Confusion Matrix: \n",
      "[[11  5]\n",
      " [ 2  6]]\n",
      "Best Threshold: 0.4004285714285715\n"
     ]
    }
   ],
   "source": [
    "print(\"Metrics from best_case:\")\n",
    "print(f\"Alpha: {best_case['alpha']}\")\n",
    "print(f\"Number of Features: {best_case['num_features']}\")\n",
    "print(f\"Selected Features: {best_case['selected_features']}\")\n",
    "print(f\"Precision-Recall AUC: {best_case['pr_auc']}\")\n",
    "print(f\"Best Precision: {best_case['best_precision']}\")\n",
    "print(f\"Best Recall: {best_case['best_recall']}\")\n",
    "print(f\"ROC AUC: {best_case['roc_auc']}\")\n",
    "print(f\"F1 Score: {best_case['f1']}\")\n",
    "print(f\"Accuracy: {best_case['accuracy']}\")\n",
    "print(f\"Confusion Matrix: \\n{np.array(best_case['confusion_matrix'])}\")\n",
    "print(f\"Best Threshold: {best_case['best_threshold']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
