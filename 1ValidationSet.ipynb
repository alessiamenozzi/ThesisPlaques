{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## esegui all'inizio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dati caricati da /Users/alessiamenozzi/Desktop/ThesisPlaques/classificator_data.h5\n",
      "Numero di pazienti: 124\n",
      "Esempio di immagini per paziente: (28, 64, 64)\n",
      "Labels: [0 1 1 0 0 0 0 1 0 0 1 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0\n",
      " 1 1 0 0 1 0 1 1 1 1 1 0 0 0 1 0 0 1 0 0 0 0 1 1 1 0 1 0 1 0 0 1 0 0 0 1 0\n",
      " 1 0 1 1 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0 0 1 0 1 0 0 0 0 0 1 1 0\n",
      " 0 0 0 0 0 0 0 1 1 0 1 0 1]\n",
      "Number of labels: 124\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import os\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_curve, roc_auc_score\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_curve, auc, average_precision_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#import pymrmr\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, roc_curve, auc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Percorso del file HDF5 da cui caricare i dati\n",
    "load_path = \"/Users/alessiamenozzi/Desktop/ThesisPlaques/classificator_data.h5\"\n",
    "#load_path = \"C:\\\\Users\\\\bsbar\\\\Desktop\\\\Tesi\\\\ThesisPlaques\\\\classificator_data.h5\"\n",
    "\n",
    "# Inizializza le liste per contenere i dati\n",
    "loaded_class_images = []\n",
    "loaded_class_labels = None\n",
    "loaded_patients = None\n",
    "\n",
    "# Apre il file HDF5 in modalità lettura\n",
    "with h5py.File(load_path, 'r') as hf:\n",
    "    # Itera su tutti i gruppi nel file HDF5\n",
    "    for group_name in hf.keys():\n",
    "        # Controlla se il nome del gruppo inizia con \"patient_\"\n",
    "        if group_name.startswith(\"patient_\"):\n",
    "            group = hf[group_name]\n",
    "            # Carica le immagini dal dataset 'images' all'interno del gruppo\n",
    "            loaded_class_images.append(np.array(group['images']))\n",
    "        elif group_name == \"labels\":\n",
    "            # Carica le etichette dal dataset 'labels'\n",
    "            loaded_class_labels = hf['labels'][:]\n",
    "        elif group_name == \"patients\":\n",
    "            # Carica i numeri dei pazienti dal dataset 'patients'\n",
    "            loaded_patients = hf['patients'][:]\n",
    "\n",
    "print(f\"Dati caricati da {load_path}\")\n",
    "print(f\"Numero di pazienti: {len(loaded_class_images)}\")\n",
    "print(f\"Esempio di immagini per paziente: {loaded_class_images[0].shape}\")\n",
    "\n",
    "\n",
    "# Carica il file CSV\n",
    "file_path = \"/Users/alessiamenozzi/Desktop/ThesisPlaques/data_rad_clin_DEF.csv\"\n",
    "#file_path = \"C:\\\\Users\\\\bsbar\\\\Desktop\\\\Tesi\\\\ThesisPlaques\\\\data_rad_clin_DEF.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Lista degli ID da escludere\n",
    "ids_to_exclude = [\"patient_TC_19\", \"patient_TC_40\", \"patient_TC_88\", \"patient_TC_150\", \"patient_TC_193\"]\n",
    "\n",
    "# Filtra il DataFrame per escludere le righe con gli ID specificati\n",
    "filtered_data = data[~data['IDs_new'].isin(ids_to_exclude)]\n",
    "\n",
    "# Estrae i valori dalla colonna 'label' del DataFrame filtrato\n",
    "labels_column = filtered_data['label']\n",
    "\n",
    "# Converte i valori della colonna 'label' in numeri interi\n",
    "labels = labels_column.astype(int).tolist()\n",
    "\n",
    "labels=np.array(labels)\n",
    "print(\"Labels:\", labels)\n",
    "print(\"Number of labels:\", len(labels))\n",
    "\n",
    "\n",
    "# Carica il modello encoder\n",
    "#encoder = load_model(\"/Users/alessiamenozzi/Desktop/ThesisPlaques/encoder_models/encoder_model32.h5\", compile=False)\n",
    "encoder = load_model(\"/Users/alessiamenozzi/Desktop/encoder64_senzaDense.h5\", compile=False)\n",
    "#encoder = load_model(\"C:\\\\Users\\\\bsbar\\\\Desktop\\\\Tesi\\\\ThesisPlaques\\\\encoder_models\\\\encoder_model64.h5\", compile=False)\n",
    "\n",
    "# Funzione per ottenere le feature da una singola immagine\n",
    "def get_features_from_image(image):\n",
    "    image = image.astype('float32')\n",
    "    image = image / 255.0\n",
    "    # Ottieni le feature dall'encoder\n",
    "    if len(image.shape) == 2:  # Se l'immagine è 64x64\n",
    "        image = np.expand_dims(image, axis=-1)  # Aggiungi canale se necessario (per immagini in bianco e nero)\n",
    "    image = np.expand_dims(image, axis=0)  # Aggiungi dimensione batch\n",
    "    features = encoder.predict(image, verbose=False)\n",
    "    features = np.squeeze(features)\n",
    "    #print(features.shape)\n",
    "    return np.array(features)\n",
    "\n",
    "# Lista per salvare le feature delle immagini\n",
    "patients = []\n",
    "\n",
    "# Ottieni le feature per ogni immagine nella lista\n",
    "for i in range(len(loaded_class_images)):\n",
    "    features_list = []\n",
    "    for img in loaded_class_images[i]:\n",
    "        features = get_features_from_image(img)\n",
    "        features_list.append(features)\n",
    "    fetures_list = np.array(features_list)\n",
    "    patients.append(features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## funzioni 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FILE DI FUNZIONI\n",
    "import h5py\n",
    "import os\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_curve, roc_auc_score\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_curve, auc, average_precision_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "#import pymrmr\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, roc_curve, auc\n",
    "\n",
    "\n",
    "def continue_array(filtered_patients, labels):\n",
    "    all_features = []\n",
    "    for patient in filtered_patients:\n",
    "        for image_features in patient:\n",
    "            all_features.append(image_features)\n",
    "\n",
    "    all_features_array = np.array(all_features)\n",
    "\n",
    "    # Print the shape of the combined features array\n",
    "    #print(f\"Shape of combined features array: {all_features_array.shape}\")\n",
    "\n",
    "    # Initialize the new expanded labels and patient IDs vectors\n",
    "    expanded_labels = []\n",
    "    expanded_patient_ids = []\n",
    "\n",
    "    # Assuming labels and loaded_patients are already defined\n",
    "    # labels: A list of length 124 with labels for each patient\n",
    "    # loaded_patients: A list of length 124 with IDs for each patient\n",
    "\n",
    "    # Iterate over the filtered patients and expand the labels and patient IDs\n",
    "    for i in range(len(filtered_patients)):\n",
    "        num_images = len(filtered_patients[i])  # Number of images for the current patient\n",
    "        expanded_labels.extend([labels[i]] * num_images)  # Assign the same label to all images of the patient\n",
    "        expanded_patient_ids.extend([loaded_patients[i]] * num_images)  # Assign the same patient ID to all images of the patient\n",
    "\n",
    "    # Convert the lists to NumPy arrays\n",
    "    expanded_labels_array = np.array(expanded_labels)\n",
    "    expanded_patient_ids_array = np.array(expanded_patient_ids)\n",
    "\n",
    "    # Print the shapes of the new arrays to verify\n",
    "    #print(f\"Shape of expanded labels array: {expanded_labels_array.shape}\")\n",
    "    #print(f\"Shape of expanded patient IDs array: {expanded_patient_ids_array.shape}\")\n",
    "    return all_features_array, expanded_labels_array, expanded_patient_ids_array\n",
    "\n",
    "\n",
    "def filter_highly_correlated_features(df, corr, threshold=0.85):\n",
    "    columns = np.full((corr.shape[0],), True, dtype=bool)\n",
    "    removed_features = []\n",
    "\n",
    "    for i in range(corr.shape[0]):\n",
    "        for j in range(i + 1, corr.shape[0]):\n",
    "            if corr.iloc[i, j] >= threshold:\n",
    "                if columns[j]:\n",
    "                    columns[j] = False\n",
    "                    removed_features.append(df.columns[j])\n",
    "\n",
    "    #selected_columns = df.columns[columns]\n",
    "    return removed_features\n",
    "\n",
    "\n",
    "\n",
    "def remove_features_from_patients(patients, features_to_remove):\n",
    "    # Estrai gli indici delle caratteristiche da rimuovere\n",
    "    feature_indices_to_remove = [int(feature.split('_')[1]) for feature in features_to_remove]\n",
    "    \n",
    "    # Rimuovi le caratteristiche corrispondenti dagli array dei pazienti\n",
    "    final_patients = []\n",
    "    for patient in patients:\n",
    "        new_patients = []\n",
    "        for image_features in patient:\n",
    "            new_patient = np.delete(image_features, feature_indices_to_remove, axis=0)\n",
    "            new_patients.append(new_patient)\n",
    "        final_patients.append(np.array(new_patients))    \n",
    "\n",
    "    return final_patients\n",
    "\n",
    "\n",
    "def perform_correlation(z_train, y_train, numero = 32, threshold = 0.85):\n",
    "    # Supponiamo che loaded_patients contenga le etichette dei pazienti\n",
    "\n",
    "    all_images, _, _= continue_array(z_train, y_train)\n",
    "\n",
    "    # Creazione del DataFrame con le immagini come righe e le features come colonne\n",
    "    df = pd.DataFrame(all_images, columns=[f'feature_{i}' for i in range(numero)])\n",
    "\n",
    "    # Visualizzazione del DataFrame\n",
    "    corr_matrix = df.corr()\n",
    "\n",
    "    # Se vuoi anche visualizzare la matrice di correlazione con Seaborn\n",
    "    #plt.figure(figsize=(12, 10))\n",
    "    #sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm')\n",
    "    #plt.title(\"Feature Correlation Matrix\")\n",
    "    #plt.show()  \n",
    "    \n",
    "    features_selected = filter_highly_correlated_features(df, corr_matrix, threshold)\n",
    "    \n",
    "    return features_selected\n",
    "\n",
    "\n",
    "\n",
    "## FEATURE SELECTION LASSO\n",
    "def select_features_with_lasso(X, y, alpha=0.001):\n",
    "    \n",
    "    # Fit Lasso regression model\n",
    "    lasso = Lasso(alpha=alpha)\n",
    "    lasso.fit(X, y)\n",
    "\n",
    "    # Get coefficients\n",
    "    coefficients = lasso.coef_\n",
    "\n",
    "    # Select features with non-zero coefficients\n",
    "    selected_features = np.where(coefficients != 0)[0]\n",
    "\n",
    "    # Create new feature matrix with only selected features\n",
    "    X_selected = X[:, selected_features]\n",
    "\n",
    "    return X_selected, selected_features\n",
    "\n",
    "## FEATURE SELECTION LOGISTIC\n",
    "def logistic_regression_feature_selection(X, y, num_features):\n",
    "    lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    lr.fit(X, y)\n",
    "    coef_abs = np.abs(lr.coef_)\n",
    "    feature_importances = np.mean(coef_abs, axis=0)\n",
    "    selected_features = feature_importances.argsort()[-num_features:][::-1]\n",
    "    # Create new feature matrix with only selected features\n",
    "    X_selected = X[:, selected_features]\n",
    "    return X_selected, selected_features\n",
    "\n",
    "## FEATURE SELECTION MRMR\n",
    "def mrmr_feature_selection(X, y, num_features):\n",
    "    feature_names = [f'feature_{i}' for i in range(X.shape[1])]\n",
    "    data = pd.DataFrame(X, columns=feature_names)\n",
    "    data['target'] = y\n",
    "    data.columns = data.columns.astype(str)\n",
    "    selected_features = pymrmr.mRMR(data, 'MIQ', num_features)\n",
    "    selected_indices = [data.columns.get_loc(feature) for feature in selected_features]\n",
    "    # Crea una nuova matrice delle caratteristiche con solo le caratteristiche selezionate\n",
    "    X_selected = X[:, selected_indices]\n",
    "\n",
    "    return X_selected, selected_indices\n",
    "\n",
    "## FEATURE SELECTION RANDOM FOREST\n",
    "def rf_feature_selection(X, y, num_features):\n",
    "    # Inizializza il classificatore Random Forest\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    # Addestra il modello\n",
    "    rf.fit(X, y)\n",
    "    # Ottieni l'importanza delle caratteristiche\n",
    "    feature_importances = rf.feature_importances_\n",
    "    # Seleziona gli indici delle caratteristiche più importanti\n",
    "    selected_features = np.argsort(feature_importances)[-num_features:][::-1]\n",
    "    # Crea una nuova matrice di caratteristiche con solo le caratteristiche selezionate\n",
    "    X_selected = X[:, selected_features]\n",
    "    return X_selected, selected_features\n",
    "\n",
    "## FUNZIONE PER RIMUOVERE FEATURES SELEZIONATE\n",
    "def filter_patients_features(filtered_patients, selected_features):\n",
    "    \"\"\"\n",
    "    Removes the non-selected features from the filtered_patients array.\n",
    "\n",
    "    Parameters:\n",
    "    filtered_patients (list of numpy.ndarray): The list containing patients' images' features.\n",
    "    selected_features (numpy.ndarray): The indices of the selected features.\n",
    "\n",
    "    Returns:\n",
    "    list of numpy.ndarray: The new filtered_patients array with only the selected features.\n",
    "    \"\"\"\n",
    "    filtered_patients_selected = []\n",
    "\n",
    "    for patient_features in filtered_patients:\n",
    "        # Select only the features specified in selected_features\n",
    "        patient_features_selected = patient_features[:, selected_features]\n",
    "        filtered_patients_selected.append(patient_features_selected)\n",
    "\n",
    "    return filtered_patients_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## funzioni 2 per la classificazione"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classificazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import ADASYN\n",
    "def prob_to_binary(predictions_proba, threshold):\n",
    "        final_predictionarray=[]          \n",
    "        ## majority voting\n",
    "        for p in predictions_proba:\n",
    "            test_patient_predictions=[]\n",
    "            for proba in p:               \n",
    "                predictions_binary = 1 if proba[0][1] > threshold else 0\n",
    "                test_patient_predictions.append(predictions_binary)\n",
    "            count_0 = np.sum(np.array(test_patient_predictions)==0) \n",
    "            count_1 = np.sum(np.array(test_patient_predictions)==1)                                   \n",
    "            final_prediction = 0 if count_0 > count_1 else 1\n",
    "            final_predictionarray.append(final_prediction)\n",
    "         \n",
    "        return final_predictionarray\n",
    "\n",
    "\n",
    "def classification_method1(selector, alpha, classifier, x_train_expanded, y_train_expanded, patients_test, y_test, features_test, num_features, thresholds=np.arange(0.001, 0.501, 0.001)):\n",
    "\n",
    "    if num_features != len(x_train_expanded[0]) or alpha != 0:\n",
    "        if selector == \"lasso\":\n",
    "            X_selected, selected_features = select_features_with_lasso(x_train_expanded, y_train_expanded, alpha)\n",
    "        elif selector == \"logistic\": \n",
    "            X_selected, selected_features = logistic_regression_feature_selection(x_train_expanded, y_train_expanded, num_features)\n",
    "        elif selector == \"mrmr\":\n",
    "            X_selected, selected_features = mrmr_feature_selection(x_train_expanded, y_train_expanded, num_features)\n",
    "        elif selector == \"rf\":\n",
    "            X_selected, selected_features = rf_feature_selection(x_train_expanded, y_train_expanded, num_features)\n",
    "        else:\n",
    "            print(\"Wrong selector. Choose between: mrmr, rf, logistic, lasso\")\n",
    "            return\n",
    "        \n",
    "        features_test = filter_patients_features(features_test, selected_features)\n",
    "\n",
    "    else:\n",
    "        X_selected = x_train_expanded\n",
    "\n",
    "    number_features=len(selected_features)\n",
    "    #print(\"X_selected[0] \", X_selected[0])\n",
    "    #print(\"X_selected[1] \", X_selected[1])\n",
    "\n",
    "    if(len(X_selected[0]))==0:\n",
    "        #print(X_selected.shape)\n",
    "        return 0,0,0,0,0,0,0,0,0,0\n",
    "    \n",
    "    #smote = SMOTE(random_state=42)\n",
    "    adasyn = ADASYN(random_state=42)\n",
    "\n",
    "    # Applicare SMOTE al set di dati di addestramento espanso\n",
    "    #X_resampled, y_resampled = smote.fit_resample(X_selected, y_train_expanded)\n",
    "    X_resampled, y_resampled = adasyn.fit_resample(X_selected, y_train_expanded)\n",
    "\n",
    "    #print(X_selected, \" vs Resampled \", X_resampled)\n",
    "    #print(y_test, \"vs resempled \" , y_resampled)\n",
    "    #classifier.fit(X_selected, y_train_expanded)\n",
    "    classifier.fit(X_resampled, y_resampled)\n",
    "\n",
    "   \n",
    "    ### questo trova le probabilità e fa la media\n",
    "    temp_array = []\n",
    "    patient_scores = []\n",
    "\n",
    "    for x in range(len(patients_test)):\n",
    "        patient_predictions = []\n",
    "        patient_predictions1 = []\n",
    "        for i in range(len(features_test[x])):\n",
    "            dato = features_test[x][i].reshape(1, -1)\n",
    "            prediction = classifier.predict_proba(dato)\n",
    "            prediction1 = classifier.predict_proba(dato)[:,1]\n",
    "            patient_predictions.append(prediction)\n",
    "            patient_predictions1.append(prediction1)\n",
    "            mean=np.mean(patient_predictions1)\n",
    "\n",
    "        patient_scores.append(mean) ## contiene la media delle probabilità\n",
    "        temp_array.append(patient_predictions) ## contiene tutte le probabilità\n",
    "    \n",
    "    best_f1_score = 0\n",
    "    best_threshold = None\n",
    "    best_precision=0\n",
    "    best_recall=0\n",
    "    best_prediction=[]\n",
    "    \n",
    "\n",
    "    # Valuta le performance utilizzando diverse threshold\n",
    "    #thresholds = np.arange(0.001, 0.501, 0.001)\n",
    "    #thresholds=[0.5]\n",
    "    if isinstance(thresholds, (int, float, complex)):\n",
    "        thresholds=[thresholds]\n",
    "\n",
    "    ## se non viene specificato usi il parametro di default \n",
    "    if(len(thresholds)!=1):\n",
    "        for threshold in thresholds:\n",
    "            binary_predictions = prob_to_binary(temp_array, threshold)\n",
    "            f1 = f1_score(y_test, binary_predictions)\n",
    "            if f1 > best_f1_score:\n",
    "                best_f1_score = f1\n",
    "                best_threshold = threshold\n",
    "                best_precision = precision_score(y_test, binary_predictions)\n",
    "                best_recall = recall_score(y_test, binary_predictions)\n",
    "                best_prediction=binary_predictions\n",
    "\n",
    "    else: ## qui vuol dire che è il set di test che usa la threshold migliore\n",
    "        best_threshold = thresholds\n",
    "        best_prediction=prob_to_binary(temp_array, best_threshold)\n",
    "        best_f1_score = f1_score(y_test, best_prediction)\n",
    "        best_precision = precision_score(y_test, best_prediction)\n",
    "        best_recall = recall_score(y_test, best_prediction)\n",
    "\n",
    "    #print(f\"La migliore threshold è {best_threshold} con f1score di {best_f1_score} e precision {best_precision} e recall {best_recall}.\")\n",
    "\n",
    "    y_test= np.array(y_test)\n",
    "    best_prediction=np.array(best_prediction)\n",
    "    test_accuracy = accuracy_score(y_test, best_prediction)\n",
    "\n",
    "\n",
    "    #test_precision = precision_score(y_test, best_prediction)\n",
    "    #test_recall = recall_score(y_test, best_prediction)\n",
    "    \n",
    "    precision, recall, _ = precision_recall_curve(y_test, patient_scores)\n",
    "    pr_auc = average_precision_score(y_test, patient_scores)\n",
    "\n",
    "    #fpr, tpr, _ = roc_curve(y_test, best_prediction)\n",
    "    roc_auc= roc_auc_score(y_test, patient_scores)\n",
    "\n",
    "\n",
    "    conf= confusion_matrix(y_test, best_prediction)\n",
    "\n",
    "    return best_f1_score, best_precision, best_recall, conf, test_accuracy, precision, recall, pr_auc, roc_auc, best_threshold, number_features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### best threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_threshold_features(results_rf, selector):\n",
    "    best_thresholds = []\n",
    "    best_num_features=[]\n",
    "    best_alpha=[]\n",
    "\n",
    "    fold_results = results_rf[selector]\n",
    "        \n",
    "        # Inizialmente, tutti gli indici con il massimo F1 score\n",
    "    best_f1_indices = np.where(fold_results['f1'] == np.max(fold_results['f1']))[0]\n",
    "        \n",
    "    if len(best_f1_indices) > 1:\n",
    "            # In caso di parità di F1, considera il pr_auc\n",
    "            max_pr_auc = max(fold_results['pr_auc'][j] for j in best_f1_indices)\n",
    "            best_f1_indices = [j for j in best_f1_indices if fold_results['pr_auc'][j] == max_pr_auc]\n",
    "            # In caso di ulteriore parità, considera l'accuracy\n",
    "            if len(best_f1_indices)!=1:\n",
    "                max_accuracy = max(fold_results['accuracy'][j] for j in best_f1_indices)\n",
    "                best_f1_indices = [j for j in best_f1_indices if fold_results['accuracy'][j] == max_accuracy]\n",
    "                if len(best_f1_indices)!=1:\n",
    "                    max_roc_auc = max(fold_results['roc_auc'][j] for j in best_f1_indices)\n",
    "                    best_f1_indices = [j for j in best_f1_indices if fold_results['roc_auc'][j] == max_roc_auc]\n",
    "                    #best_f1_indices= best_f1_indices[0]\n",
    "           \n",
    "    best_index = best_f1_indices[0]\n",
    "\n",
    "\n",
    "    # Ottieni l'indice finale del miglior F1 score\n",
    "    best_threshold=fold_results['best_threshold'][best_index]\n",
    "\n",
    "    if selector == 'lasso':\n",
    "        mean_param=fold_results['alpha'][best_index]\n",
    "    else:\n",
    "        mean_param=fold_results['num_features'][best_index]\n",
    "\n",
    "\n",
    "\n",
    "    return best_threshold, mean_param\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Supponiamo che tu abbia un elenco di colori predefiniti per i fold\n",
    "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n",
    "\n",
    "# Supponiamo che results_rf sia un dizionario di dizionari dove:\n",
    "# results_rf[i][selector] contiene i dati per il fold i e il selettore selector.\n",
    "# results_test_rf contiene i dati per il set di test finale per ogni selector.\n",
    "\n",
    "def plot_results(results_rf, results_test_rf, selector):\n",
    "\n",
    "    # Plotta i risultati per PR AUC\n",
    "    if selector == 'lasso':\n",
    "        \n",
    "        plt.figure()\n",
    "        for i in range(len(results_rf)):\n",
    "            fold_results = results_rf[i][selector]\n",
    "            plt.plot(fold_results['alpha'], fold_results['pr_auc'], marker='o', linestyle='-', color=colors[i % len(colors)], label=f'Fold {i+1}')\n",
    "        \n",
    "        # Plot dei risultati del set di test\n",
    "        plt.plot(results_test_rf[selector]['alpha'], results_test_rf[selector]['pr_auc'], marker='x', linestyle='--', color='black', label='Test Set')\n",
    "        \n",
    "        plt.xlabel('Alpha Values')\n",
    "        plt.ylabel('AUC')\n",
    "        plt.title(f'Alpha Values vs PR AUC for {selector} selector')\n",
    "        plt.xscale('log')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # Plotta i risultati per ROC AUC\n",
    "        plt.figure()\n",
    "        for i in range(len(results_rf)):\n",
    "            fold_results = results_rf[i][selector]\n",
    "            plt.plot(fold_results['alpha'], fold_results['roc_auc'], marker='o', linestyle='-', color=colors[i % len(colors)], label=f'Fold {i+1}')\n",
    "        \n",
    "        # Plot dei risultati del set di test\n",
    "        plt.plot(results_test_rf[selector]['alpha'], results_test_rf[selector]['roc_auc'], marker='x', linestyle='--', color='black', label='Test Set')\n",
    "        \n",
    "        plt.xlabel('Alpha Values')\n",
    "        plt.ylabel('AUC')\n",
    "        plt.title(f'Alpha Values vs ROC AUC for {selector} selector')\n",
    "        plt.xscale('log')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    else:\n",
    "        plt.figure()\n",
    "        for i in range(len(results_rf)):\n",
    "            fold_results = results_rf[i][selector]\n",
    "            plt.plot(fold_results['num_features'], fold_results['pr_auc'], marker='o', linestyle='-', color=colors[i % len(colors)], label=f'Fold {i+1}')\n",
    "        \n",
    "        # Plot dei risultati del set di test\n",
    "        plt.plot(results_test_rf[selector]['num_features'], results_test_rf[selector]['pr_auc'], marker='x', linestyle='--', color='black', label='Test Set')\n",
    "        \n",
    "        plt.xlabel('Number of Features')\n",
    "        plt.ylabel('AUC')\n",
    "        plt.title(f'Number of Features vs PR AUC for {selector} selector')\n",
    "        plt.xscale('linear')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # Plotta i risultati per ROC AUC\n",
    "        plt.figure()\n",
    "        for i in range(len(results_rf)):\n",
    "            fold_results = results_rf[i][selector]\n",
    "            plt.plot(fold_results['num_features'], fold_results['roc_auc'], marker='o', linestyle='-', color=colors[i % len(colors)], label=f'Fold {i+1}')\n",
    "        \n",
    "        # Plot dei risultati del set di test\n",
    "        plt.plot(results_test_rf[selector]['num_features'], results_test_rf[selector]['roc_auc'], marker='x', linestyle='--', color='black', label='Test Set')\n",
    "        \n",
    "        plt.xlabel('Number of Features')\n",
    "        plt.ylabel('AUC')\n",
    "        plt.title(f'Number of Features vs ROC AUC for {selector} selector')\n",
    "        plt.xscale('linear')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## codice con split e cross val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(69,)\n",
      "[102  70  95  53 111  78 113 115 129  33  61  27  75 104 146 133 120  84\n",
      " 107  23  24  39 136 135  68  94  81  44  42  15  48 126  41  91  54  35\n",
      " 117  17  65  89  98  82  96  90 119 139 127  43 112  92 118  59 100 144\n",
      " 106 101  62 108  87  74  71  22 128 137 116  52 124 103 105]\n",
      "[147 149 153 155 158 159 161 163 166 168 169 170 171 175 176 178 182 183\n",
      " 188 189 190 197 199 200 205]\n",
      "69\n",
      "64\n",
      "38\n",
      "22\n",
      "(25,)\n"
     ]
    }
   ],
   "source": [
    "#patients_train, patients_test, y_train, y_test, features_train, features_test= train_test_split(loaded_patients, labels, patients, test_size=0.2, shuffle=True, stratify=labels, random_state=46)\n",
    "\n",
    "patients_train, patients_test, y_train, y_test, features_train, features_test= train_test_split(loaded_patients, labels, patients, test_size=0.2, shuffle=False, random_state=1)\n",
    "patients_train, patients_val, y_train, y_val, features_train, features_val= train_test_split(patients_train, y_train, features_train, test_size=0.3, shuffle=True, stratify=y_train, random_state=1)\n",
    "\n",
    "print(patients_train.shape)\n",
    "print(patients_train)\n",
    "print(patients_test)\n",
    "print(len(features_train))\n",
    "print(len(features_train[0][0]))\n",
    "print(len(features_train[1]))\n",
    "print(len(features_train[2]))\n",
    "print(patients_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "alpha_values = np.linspace(0, 0.006, 30).tolist()\n",
    "\n",
    "#selectors=['mrmr', 'rf', 'logistic', 'lasso']\n",
    "selectors=['rf', 'lasso', 'logistic']\n",
    "classifiers=['RandomForest', 'Logistic', 'XgBoost', 'MLP', 'SVM']\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOOP con tutti i selector e classificatori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected features [37 15]\n",
      "selected features [37 15 13]\n",
      "selected features [37 15 13  3]\n",
      "selected features [37 15 13  3  1]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 92\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(classifier\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMLP\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     90\u001b[0m      classi \u001b[38;5;241m=\u001b[39m MLPClassifier(hidden_layer_sizes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m128\u001b[39m,\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m32\u001b[39m), max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, early_stopping\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madaptive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 92\u001b[0m best_f1_score, best_precision, best_recall, conf, test_accuracy, precision, recall, pr_auc, roc_auc, bt\u001b[38;5;241m=\u001b[39m \u001b[43mclassification_method1\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train_expanded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_expanded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatients_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_patients_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m results_val[i][selector][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_features\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(t)\n\u001b[1;32m     98\u001b[0m results_val[i][selector][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpr_auc\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(pr_auc)\n",
      "Cell \u001b[0;32mIn[29], line 56\u001b[0m, in \u001b[0;36mclassification_method1\u001b[0;34m(selector, alpha, classifier, x_train_expanded, y_train_expanded, patients_test, y_test, features_test, num_features, thresholds)\u001b[0m\n\u001b[1;32m     51\u001b[0m X_resampled, y_resampled \u001b[38;5;241m=\u001b[39m adasyn\u001b[38;5;241m.\u001b[39mfit_resample(X_selected, y_train_expanded)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m#print(X_selected, \" vs Resampled \", X_resampled)\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m#print(y_test, \"vs resempled \" , y_resampled)\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m#classifier.fit(X_selected, y_train_expanded)\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m \u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_resampled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_resampled\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m### questo trova le probabilità e fa la media\u001b[39;00m\n\u001b[1;32m     60\u001b[0m temp_array \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:489\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    478\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    481\u001b[0m ]\n\u001b[1;32m    483\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 489\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     66\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/parallel.py:129\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:192\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    190\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[0;32m--> 192\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     tree\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[1;32m    201\u001b[0m         X,\n\u001b[1;32m    202\u001b[0m         y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    205\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[1;32m    206\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/tree/_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    463\u001b[0m         splitter,\n\u001b[1;32m    464\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    470\u001b[0m     )\n\u001b[0;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "results_test=[{} for _ in range(len(classifiers))]\n",
    "results_val = [{} for _ in range(len(classifiers))]\n",
    "i=0\n",
    "for classifier in classifiers:\n",
    "    for selector in selectors: \n",
    "        results_test[i][selector] = {\n",
    "                'classifier': classifier,\n",
    "                'alpha': [],\n",
    "                'num_features': [],\n",
    "                'pr_auc': [],\n",
    "                'best_precision' :[],\n",
    "                'best_recall':[],\n",
    "                'roc_auc':[],\n",
    "                'f1':[],\n",
    "                'accuracy':[],\n",
    "                'confusion_matrix':[],\n",
    "                'best_threshold':[]\n",
    "                }\n",
    "        results_val[i][selector] = {\n",
    "                'classifier': classifier,\n",
    "                'alpha': [],\n",
    "                'num_features': [],\n",
    "                'pr_auc': [],\n",
    "                'best_precision' :[],\n",
    "                'best_recall':[],\n",
    "                'roc_auc':[],\n",
    "                'f1':[],\n",
    "                'accuracy':[],\n",
    "                'confusion_matrix':[],\n",
    "                'best_threshold':[]\n",
    "                }\n",
    "\n",
    "\n",
    "        features=perform_correlation(features_train, y_train, 64, 0.85)\n",
    "\n",
    "        final_patients_train=remove_features_from_patients(features_train, features)\n",
    "        final_patients_test=remove_features_from_patients(features_test, features)\n",
    "        final_patients_val=remove_features_from_patients(features_val, features)\n",
    "\n",
    "        x_train_expanded, y_train_expanded, _ = continue_array(final_patients_train, y_train)\n",
    "\n",
    "        \n",
    "        if selector == 'lasso':\n",
    "                for alpha in alpha_values:\n",
    "                        if(classifier=='RandomForest'):\n",
    "                             classi = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "                        if(classifier=='Logistic'):\n",
    "                             classi = LogisticRegression()\n",
    "                        if(classifier=='SVM'):\n",
    "                             classi = SVC(kernel='rbf', probability=True, random_state=42)\n",
    "                        if(classifier=='XgBoost'):\n",
    "                             classi = XGBClassifier()\n",
    "                        if(classifier=='MLP'):\n",
    "                             classi = MLPClassifier(hidden_layer_sizes=(128,64, 32), max_iter=1000, random_state=42, early_stopping=True, learning_rate='adaptive')\n",
    "                        \n",
    "                        best_f1_score, best_precision, best_recall, conf, test_accuracy, precision, recall, pr_auc, roc_auc, bt, number_features= classification_method1(\n",
    "                            selector, alpha, classi, x_train_expanded, y_train_expanded, \n",
    "                            patients_val, y_val, final_patients_val, 0\n",
    "                        )\n",
    "\n",
    "                        if(best_f1_score==0 and best_precision==0 and best_recall==0):\n",
    "                            break\n",
    "                    \n",
    "                        results_val[i][selector]['alpha'].append(alpha)\n",
    "                        results_val[i][selector]['num_features'].append(number_features)\n",
    "                        results_val[i][selector]['pr_auc'].append(pr_auc)\n",
    "                        results_val[i][selector]['best_precision'].append(best_precision)\n",
    "                        results_val[i][selector]['best_recall'].append(best_recall)\n",
    "                        results_val[i][selector]['roc_auc'].append(roc_auc)\n",
    "                        results_val[i][selector]['f1'].append(best_f1_score)\n",
    "                        results_val[i][selector]['accuracy'].append(test_accuracy)\n",
    "                        results_val[i][selector]['confusion_matrix'].append(conf)\n",
    "                        results_val[i][selector]['best_threshold'].append(bt)\n",
    "\n",
    "                        # Plot dei risultati per 'lasso'\n",
    "                        \n",
    "\n",
    "\n",
    "        else:\n",
    "                    for t in range(2, len(x_train_expanded[0])+1):\n",
    "\n",
    "                        if(classifier=='RandomForest'):\n",
    "                             classi = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "                        if(classifier=='Logistic'):\n",
    "                             classi = LogisticRegression()\n",
    "                        if(classifier=='SVM'):\n",
    "                             classi = SVC(kernel='rbf', probability=True, random_state=42)\n",
    "                        if(classifier=='XgBoost'):\n",
    "                             classi = XGBClassifier()\n",
    "                        if(classifier=='MLP'):\n",
    "                             classi = MLPClassifier(hidden_layer_sizes=(128,64, 32), max_iter=1000, random_state=42, early_stopping=True, learning_rate='adaptive')\n",
    "\n",
    "                        best_f1_score, best_precision, best_recall, conf, test_accuracy, precision, recall, pr_auc, roc_auc, bt, _= classification_method1(\n",
    "                            selector, 0, classi, x_train_expanded, y_train_expanded, \n",
    "                            patients_val, y_val, final_patients_val, t\n",
    "                        )\n",
    "\n",
    "                        results_val[i][selector]['num_features'].append(t)\n",
    "                        results_val[i][selector]['pr_auc'].append(pr_auc)\n",
    "                        results_val[i][selector]['best_precision'].append(best_precision)\n",
    "                        results_val[i][selector]['best_recall'].append(best_recall)\n",
    "                        results_val[i][selector]['roc_auc'].append(roc_auc)\n",
    "                        results_val[i][selector]['f1'].append(best_f1_score)\n",
    "                        results_val[i][selector]['accuracy'].append(test_accuracy)\n",
    "                        results_val[i][selector]['confusion_matrix'].append(conf)\n",
    "                        results_val[i][selector]['best_threshold'].append(bt)\n",
    "\n",
    "\n",
    "        mean_threshold, mean_param  = find_best_threshold_features(results_val[i], selector)\n",
    "        print(f\"The mean best threshold for {selector} is: {mean_threshold}\")\n",
    "    \n",
    "        if selector=='lasso': ## sul set di test prendo solo il parametro migliore \n",
    "            for alpha in alpha_values:\n",
    "                if(classifier=='RandomForest'):\n",
    "                             classi = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "                if(classifier=='Logistic'):\n",
    "                             classi = LogisticRegression()\n",
    "                if(classifier=='SVM'):\n",
    "                             classi = SVC(kernel='rbf', probability=True, random_state=42)\n",
    "                if(classifier=='XgBoost'):\n",
    "                             classi = XGBClassifier()\n",
    "                if(classifier=='MLP'):\n",
    "                             classi = MLPClassifier(hidden_layer_sizes=(128,64, 32), max_iter=1000, random_state=42, early_stopping=True, learning_rate='adaptive')\n",
    "                \n",
    "                best_f1_score, best_precision, best_recall, conf, test_accuracy, precision, recall, pr_auc, roc_auc, bt, number_features= classification_method1(\n",
    "                                    selector, alpha, classi, x_train_expanded, y_train_expanded, \n",
    "                                    patients_test, y_test, final_patients_test, 0, mean_threshold)\n",
    "                if(best_f1_score==0 and best_precision==0 and best_recall==0):\n",
    "                    break\n",
    "\n",
    "                results_test[i][selector]['alpha'].append(alpha)\n",
    "                results_test[i][selector]['num_features'].append(number_features)\n",
    "                results_test[i][selector]['pr_auc'].append(pr_auc)\n",
    "                results_test[i][selector]['best_precision'].append(best_precision)\n",
    "                results_test[i][selector]['best_recall'].append(best_recall)\n",
    "                results_test[i][selector]['roc_auc'].append(roc_auc)\n",
    "                results_test[i][selector]['f1'].append(best_f1_score)\n",
    "                results_test[i][selector]['accuracy'].append(test_accuracy)\n",
    "                results_test[i][selector]['confusion_matrix'].append(conf)\n",
    "                results_test[i][selector]['best_threshold'].append(bt)\n",
    "\n",
    "            if(classifier=='RandomForest'):\n",
    "                             classi = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "            if(classifier=='Logistic'):\n",
    "                             classi = LogisticRegression()\n",
    "            if(classifier=='SVM'):\n",
    "                             classi = SVC(kernel='rbf', probability=True, random_state=42)\n",
    "            if(classifier=='XgBoost'):\n",
    "                             classi = XGBClassifier()\n",
    "            if(classifier=='MLP'):\n",
    "                             classi = MLPClassifier(hidden_layer_sizes=(128,64, 32), max_iter=1000, random_state=42, early_stopping=True, learning_rate='adaptive')\n",
    "            \n",
    "            best_f1_score, best_precision, best_recall, conf, test_accuracy, precision, recall, pr_auc, roc_auc, bt, number_features= classification_method1(\n",
    "                                    selector, mean_param, classi, x_train_expanded, y_train_expanded, \n",
    "                                    patients_test, y_test, final_patients_test, 0, mean_threshold\n",
    "                                )\n",
    "            \n",
    "            print(f\"Results obtained with the best mean parameters on the test set for {selector} selector and {classifier} classifier \\n\")\n",
    "            print(\"alpha: \", mean_param)\n",
    "            print(\"Features: \", number_features)\n",
    "            print(\"threshold: \", mean_threshold)\n",
    "            print(\"F1 score: \", best_f1_score)\n",
    "            print(\"best_precision: \", best_precision)\n",
    "            print(\"best_recall: \", best_recall)\n",
    "            print(\"confusion matrix : \", conf)\n",
    "            print(\"test_accuracy: \", test_accuracy)\n",
    "            print(\"pr_auc: \", pr_auc)\n",
    "            print(\"roc_auc: \", roc_auc)\n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "        else:\n",
    "\n",
    "            for t in range(2, len(x_train_expanded[0])+1):\n",
    "                if(classifier=='RandomForest'):\n",
    "                             classi = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "                if(classifier=='Logistic'):\n",
    "                             classi = LogisticRegression()\n",
    "                if(classifier=='SVM'):\n",
    "                             classi = SVC(kernel='rbf', probability=True, random_state=42)\n",
    "                if(classifier=='XgBoost'):\n",
    "                             classi = XGBClassifier()\n",
    "                if(classifier=='MLP'):\n",
    "                             classi = MLPClassifier(hidden_layer_sizes=(128,64, 32), max_iter=1000, random_state=42, early_stopping=True, learning_rate='adaptive')\n",
    "\n",
    "                best_f1_score, best_precision, best_recall, conf, test_accuracy, precision, recall, pr_auc, roc_auc, bt, _= classification_method1(\n",
    "                                    selector, 0, classi, x_train_expanded, y_train_expanded, \n",
    "                                    patients_test, y_test, final_patients_test, t, mean_threshold )\n",
    "\n",
    "                results_test[i][selector]['num_features'].append(t)\n",
    "                results_test[i][selector]['pr_auc'].append(pr_auc)\n",
    "                results_test[i][selector]['best_precision'].append(best_precision)\n",
    "                results_test[i][selector]['best_recall'].append(best_recall)\n",
    "                results_test[i][selector]['roc_auc'].append(roc_auc)\n",
    "                results_test[i][selector]['f1'].append(best_f1_score)\n",
    "                results_test[i][selector]['accuracy'].append(test_accuracy)\n",
    "                results_test[i][selector]['confusion_matrix'].append(conf)\n",
    "                results_test[i][selector]['best_threshold'].append(bt)\n",
    "\n",
    "            if(classifier=='RandomForest'):\n",
    "                             classi = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "            if(classifier=='Logistic'):\n",
    "                             classi = LogisticRegression()\n",
    "            if(classifier=='SVM'):\n",
    "                             classi = SVC(kernel='rbf', probability=True, random_state=42)\n",
    "            if(classifier=='XgBoost'):\n",
    "                             classi = XGBClassifier()\n",
    "            if(classifier=='MLP'):\n",
    "                             classi = MLPClassifier(hidden_layer_sizes=(128,64, 32), max_iter=1000, random_state=42, early_stopping=True, learning_rate='adaptive')\n",
    "\n",
    "            best_f1_score, best_precision, best_recall, conf, test_accuracy, precision, recall, pr_auc, roc_auc, bt, _= classification_method1(\n",
    "                                    selector, mean_param, classi, x_train_expanded, y_train_expanded, \n",
    "                                    patients_test, y_test, final_patients_test, 0, mean_threshold\n",
    "                                )\n",
    "            \n",
    "            print(f\"Results obtained with the best mean parameters on the test set for {selector} selector and {classifier} classifier \\n\")\n",
    "            print(\"Features: \", mean_param)\n",
    "            print(\"threshold: \", mean_threshold)\n",
    "            print(\"F1 score: \", best_f1_score)\n",
    "            print(\"best_precision: \", best_precision)\n",
    "            print(\"best_recall: \", best_recall)\n",
    "            print(\"confusion matrix : \", conf)\n",
    "            print(\"test_accuracy: \", test_accuracy)\n",
    "            print(\"pr_auc: \", pr_auc)\n",
    "            print(\"roc_auc: \", roc_auc)\n",
    "            \n",
    "    i=i+1\n",
    "\n",
    "\n",
    " \n",
    "        #plot_results(results_rf, results_test_rf, selector)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
